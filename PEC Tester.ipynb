{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"italian\")\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = 'C:/ML Test/inbox/'\n",
    "file_dict = {} # Create an empty dict\n",
    "\n",
    "# Select only files with the ext extension\n",
    "#txt_files = [i for i in os.listdir(direc) if os.path.splitext(i)[1] == ext]\n",
    "\n",
    "# Iterate over your txt files\n",
    "for f in os.listdir(path):\n",
    "    # Open them and assign them to file_dict\n",
    "    with open(os.path.join(path,f), encoding=\"UTF-8\") as file_object:\n",
    "        file_dict[f] = file_object.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for file in file_dict:\n",
    "    #file_dict[file] = file_dict[file].decode('utf-8')\n",
    "    file_dict[file] = file_dict[file][file_dict[file].find('.pst'):]\n",
    "    file_dict[file] = file_dict[file].replace('\\n', '')\n",
    "    file_dict[file] = file_dict[file].split(\"Original Message\")[0]\n",
    "    file_dict[file] = file_dict[file].split(\".com\")[0]\n",
    "    file_dict[file] = file_dict[file].replace('.pst', '')\n",
    "    file_dict[file] = file_dict[file].replace('-----', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename :4\n",
      "Utile per pagare su internet. Sarebbe meglio se si potessero versare anche gli assegni sulla carta.\n",
      "Filename :1\n",
      "Dovrebbero eliminare delle formalità ossia meno burocrazia per la sottoscrizione della carta che richiede un processo molto lungo.\n",
      "Filename :6\n",
      "per gli acquisti on line,per conservare i soldi e non doverli tenere in tasca \n",
      "Filename :3\n",
      "Dovrebbero diminuire i costi sia per la ricarica che per il prelievo, essendo io correntista vorrei spendere cinquanta centesimi. \n",
      "Filename :5\n",
      "Su alcuni siti internet viene richiesto un ulteriore codice di sicurezza e la cosa è un po' noiosa perché allunga la procedura d'acquisto. \n",
      "Filename :7\n",
      "Comoda per gli acquisti online e all'estero, dato che non utilizzando le mie carte di credito evito che le clonino.\n",
      "Filename :2\n",
      "Dovrebbe implementare servizi online più velocità più semplicità. Qualsiasi cosa legata alle Poste era troppo complicato per cui ho chiuso anche il conto alle poste.\n"
     ]
    }
   ],
   "source": [
    "for i in file_dict:\n",
    "    print(\"Filename :\" + i)\n",
    "    print(file_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for file in file_dict:\n",
    "    allwords_stemmed = tokenize_and_stem(file) #for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    #allwords_tokenized = tokenize_only(file)\n",
    "    #totalvocab_tokenized.extend(allwords_tokenized)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words_italian = pd.read_csv(\"C:/Users/vimalromeo.thottumga/Documents/stopwords_new.txt\", header = 0\n",
    "                                 ,encoding ='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words_italian = stop_words_italian['stop_word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words=stop_words_italian,\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(file_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acquist',\n",
       " 'cart',\n",
       " 'cos',\n",
       " 'dovrebber',\n",
       " 'gli',\n",
       " 'gli acquist',\n",
       " 'i',\n",
       " 'internet',\n",
       " 'non',\n",
       " 'onlin']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 22 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
